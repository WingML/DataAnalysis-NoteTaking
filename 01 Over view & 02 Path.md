# 三个版块
	• 数据采集
	• 数据挖掘
	• 数据可视化

# 数据采集？
	1. 数据源：开源，爬虫，日志采集，传感器
	2. 工具：八爪鱼，火车采集器，搜集客；Python：Selenium, Ixml, Scrapy, Phantomjs
	3. 实战：微博评论抓取；下载明星海报；微博加粉丝

# 数据挖掘？
	1. 数学基础：概率论与数理统计，线性代数，图论，最优化
	2. 基本流程：商业理解，数据理解，数据准备，模型建立，模型评估，上线发布
	3. 经典算法：
		a. 分类：C4.5（信息增益比；ID3是信息增益，CART是基尼系数），Naïve Bayes，SVM，KNN，AdaBoost，CART
		b. 聚类：K-Means（ sklearn 中可以用改进版的 k-means++，EM
		c. 关联分析：Apriori
		d. 连接分析/社交网络：PageRank（还有一个HITS算法）
	4. 实战：识别手写数字；乳腺癌检测；文档归类

# 数据可视化？
	1. 工具：微图，DataV，Data GIF Maker
	2. Python：Matplotlib, Seaborn

# 修炼指南
	1. MAS 学习法：
		a. Multi-Dimension：要想方设法从多个维度去认识一个事物。就像这个专栏分成了【基础概念】【工具】【题库】三个部分，从基础原理到实操能力到场景应用多维度审视新知。
		b. Ask：不懂就要挨打，不想挨打就多问，在留言区也好，发邮件向大牛咨询也好，向身边的好友请教也好，总之尽量避免独自想一个问题想上老半天。
		c. Sharing：最好的学习就是输出与分享。写博客，写留言，做笔记，开讲座，开直播，录视频，写公众号等等，找到一个最合适的渠道，将学过的东西按自己的感觉整理出一个独有的体系，掌握的程度会远比直接接受已有的体系好许多。分享的过程会很耗时间很痛苦，但熟练就好，而且长远来看它还是你积累个人影响力的重要渠道！
	2. 认知模型：实战——工具——认知
		a. 先自下而上，从认知到实战，带着概念做总结
		b. 后自上而下，从实战回归认识，带着疑问分解出答案
	3. 三个牢记：
		a. 不重复造轮子。尽量搜寻已有的工具来实现自己的功能，许多想法没必要自己从头写代码，牛人们都帮你写好了，考虑得还比你周全，你的任务就是找到这个工具并学会使用。
		b. 工具决定效率。接上一条，怎么选择工具？答案是选择那些最多人使用的工具，因为这通常反映了它的 Bug 较少，文档较成熟，可参考的案例较多。
		c. 唯快与熟练不破。对工具的使用需要不断通过实操来掌握具体的操作步骤、传入的参数、适合的应用场景等等。

# 数据挖掘的学习路径——确定知识清单，逐个打勾
	1. 基本流程：
		a. 商业理解：从项目需求出发，定义数据挖掘的预期目标
		b. 数据理解：尝试预采集部分数据，并验证数据的质量，初步了解数据的基本统计量
		c. 数据准备：正式采集数据，并按需进行数据清洗、集成等预处理操作
		d. 模型建立：根据经验，尽可能尝试所有可能的数据挖掘模型
		e. 模型评估：分类、聚类、回归都有既定的模型评估指标，按照业务需要从中选择合适的
		f. 上线发布：数据挖掘最终的目的是转化为生产力或者用户需要的产品，比如一份报告，一个可重复的数据挖掘程序（方便后续的监控和维护）
	2. 十大经典算法——ICDM（the IEEE International Conference on Data Mining）评选出的：
		a. 分类
			i. C4.5：决策树算法，创造性地在树的构造过程中就进行了剪枝；在 ID3 的基础上改进，使用信息增益比率所有节点分裂的依据，并且可以处理连续数据、甚至不完整的数据。
			ii. Naïve Bayes：基于概率论，预先人为设定一个先验概率，根据收集到的样本统计出各个样本出现在各个标签/类别下的概率，以概率最大的标签/类别作为最终判断的分类结果
			iii. SVM：支持向量机，在不同类别之间建立一个与所有样本距离总和最大的超平面，可以是线性的平面，也可以是非线性平面；通常是对二分类而言，而如果是多分类的话基本思路是在两两类别之间建立一个“临时”的超平面
			iv. KNN：K 近邻，为每个样本选择其最相似的 K 个样本作为邻居，按照其邻居中次数最多的类别作为该样本的判断分类，通常使用欧氏距离来定义样本间的相似程度
			v. AdaBoost：串联式的集成分类器，用若干个弱分类器串联出强分类器；每个弱分类器随机从样本中采样用于训练，每个样本被赋予不同的采样权重；每个弱分类器的分类结果都会对样本的权重、以及该分类器在最终的强分类器中的权重造成影响
			vi. CART：分类和回归树，同样是决策树，同时构造一颗分类树和一颗回归树，使用 GINI 指数 作为节点分裂的依据
		b. 聚类
			i. K-Means：将样本划分为 K 类，初始时随机选定 K 个类/簇中心，每一轮迭代中将每个样本归到距离最近的簇，将在这个新的簇中用取均值的方法确定新的簇中心，迭代直到簇中心几乎不变为止
			ii. EM：最大期望算法，使用了最大似然估计；假设我们需要评估两个未知的、但是有相关性的参数 A 和 B，可以先给 A 预设一个值，基于条件概率得到 B 的新值，再反过来用基于 B 的条件概率更新 A 的值，如此反复迭代直至收敛
		c. 关联分析
			i. Apriori：寻找数据中的频繁项集，用于反映项集上各项特征的相关关系；在商业挖掘和网络安全等领域中有广泛的应用
		d. 连接分析
			i. PageRank：起源于论文影响力的计算，现被广泛用于网页重要性的衡量；基本思想是如果一个网页被越多的网页链出那这个网页就越重要，如果如果一个网页被越重要的网页链出那这个网页也越重要

# 数据挖掘的数学原理——需要掌握的数学基础
	1. 概率论与数理统计：概率论在数据挖掘中应用较多，比如朴素贝叶斯；最常用的是条件概率，独立性，随机变量
	2. 线性代数：向量、矩阵、特征值、特征向量在特征工程中有大应用，如 PCA，SVD，MF，NMF
	3. 图论：社交网络挖掘的基础就是图论
	4. 最优化：所有机器学习的问题基本可以化归为最优化问题，目的是在尽量短的时间内得到尽量好的收敛效果
