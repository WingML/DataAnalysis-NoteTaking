# 数据采集综述

- 数据采集的必要性：一个数据的走势需要足够长的时间来反映；一个数据的走势受多个维度影响。因此多源地、多维地、多数地、高质地收集数据对分析结果可能有很重要的影响。
- 数据源：
○ 开放数据源：
	§ 单位维度——政府，企业，学校；行业维度——交通，金融，能源等。
	§ 国外的开源数据通常较好，推荐的单位维度数据源如下图 ![opensource](https://raw.githubusercontent.com/WingML/DataAnalysis-NoteTaking/master/pictures/08.jpg)
○ 爬虫：
	§ 网页，app，需要格外关注版权和反爬虫机制；
	§ 可使用 python 爬虫（Requests, Xpath, Pandas; Selenium, PhantomJS, Puppteteer），也可用第三方工具（火车采集器——从数据采集到可视化一条龙；八爪鱼——付费版有云采集功能和自动切换 IP 功能；集搜客——完成可视化操作，但无云采集）。
○ 日志采集：
	§ 前端采集，后端脚本；
	§ 最大的作用是通过分析用户的访问情况，提升系统性能，提高系统承载量；适用于运维监控、安全审计、业务数据分析等场景；
	§ 按采集内容可分为【来访人 IP +时间+渠道+操作；系统错误信息；用户代理】等等；
	§ 按采集类型可分为【访问日志，错误日志】等；
	§ 按采集形式可分为【通过 Web 服务器采集】和【自定义采集用户行为】。前者如 httpd, Nginx, Tomcat 都自带日志记录功能，Hadoop 的 Chukwa, Cloudera 的 Flume, Facebook 的 Scibe 等均采用分布式架构，能满足每秒百兆级别的日志数据采集和传输需求；后者如用 JS 监听用户行为，AJAX 异步请求后台记录日志。
	§ 日志采集的关键：**埋点**，即在有需要的位置采集相应信息进行上报。除了主营核心业务外，原则上建议使用第三方的工具（友盟，Google Analysis，Talkingdata 等通过前端埋点可以看到表面的用户行为数据）
○ 传感器：图像，测速，热敏
- 数据使用原则：
○ 尽量使用开放数据（如 Kaggle 上也有许多数据）
○ 若需要爬虫，尽量使用第三方工具
- 思考题：假如想预测比特币的未来走势，需要哪些维度的数据源？如何收集？
○ 所需维度：
	§ 价格相关：历史价格走势（日/周/月）
	§ 交易相关：历史交易数据（频度，额度，交易时长，渠道，是否成功，手续费？）
	§ 成本相关：挖币成本，挖币时长
	§ 政策相关：比特币底层协议的变更，政府的政策限制/支持，新闻热度
○ 收集方式：
	§ 开源：Kaggle，百度指数等商业网站，政府官网公告
	§ 爬虫：各类新闻网站，各类金融交易平台

___

# 八爪鱼采集微博的“D&G”评论

## 用法

- 自定义任务：比较灵活
- 简易采集：集成了常用的模板，上手简单，适合赶时间的时候用，但是对于技术的提高无益

## 自定义任务的基本流程

- 输入网页：可以有多个网页，一般建议将多个网页整合进一个文件
- 设计流程：
○ 基本步骤：打开网页，点击元素，循环翻页，提取数据
○ 高级步骤：输入文字，验证码识别，下拉选项，判断条件，移动鼠标到元素上，结束循环，结束流程
- 启动采集：免费模式只有【本地采集】，更高级的功能需要付费；页面上可以看到实时的采集画面，方便查错，在完成采集任务/长时间没有回应后会结束采集，并提示保存为 xlsx/csv 文件

## 采集微博上的“Dolce&Gabbana”评论

- 为了显示完整的搜索结果，需要先登录，登录教程可以参考八爪鱼的官网，如 [这一篇](https://www.bazhuayu.com/tutorial/cookie70)
- **输入网页**：https://s.weibo.com/
- **输入关键词**：将鼠标移到输入框中，点击后右侧的操作框会出现【输入文本】的选项，点击后输入搜索内容“Dolce&Babbana”
- **点击搜索**：点击‘搜索’，将在右侧框中选中操作【点击元素】
- **设置翻页**：因为我们的收集过程是需要翻页的，而翻页需要用【循环翻页】操作，而【循环翻页】操作需要在数据提取等操作之前。一句话：一定是先设置循环翻页，再在循环中进行数据提取等操作
- **提取数据**：由于有多个字段，所以有多种操作方式：一种是先点击单个字段，比如【用户名】，正常情况下右侧框中会出现一个待采集的类似于表格的字段表，此时再依次点击要收集的其他字段，比如【微博内容，发表时间】等等，这些字段会自动补进字段表中，不要的字段可以在表格找到删除按钮；另一种操作方式是选择单条微博内容的最大目标区域，记住这一步是最容易出问题，一定要等到右侧框出现【选中子元素】才说明你选到位了。这两种操作的共同点在于，选中时会提示页面中还有 xx 个相同元素，请务必【选中全部】才能将其收集（虽然八爪鱼官网上其实还有其它的可行操作，但这样做个人感觉是最快的）
- **启动采集**：确认已经选好所有的字段后（如果你够熟悉的话，不难发现在上一步中字段的名称可以修改成你要的样子），就会提示【启动本地采集】【启动云采集】【设置定时采集】三种方式，按需。
- 具体的操作可以参见 [这一篇](https://www.bazhuayu.com/tutorial/wbplcj-7)

## 两个重要的工具

- 流程视图：方便调整/删除步骤；在数据提取的步骤中，所提取的字段都可以在此看到并删除不需要的冗余字段/修改名称
- XPath：在循环相关的操作中常用，比如在豆瓣评论的采集中，长文需要先点击【展开】，而【展开】这一个操作经常需要通过 XPath 来对 HTML 代码进行定位了，这个需要在实践上边看官方教程边慢慢摸索。比如下面三张图是我尝试爬取海王相关评论的流程图（https://movie.douban.com/subject/3878007/reviews?start=0 ）![1](https://raw.githubusercontent.com/WingML/DataAnalysis-NoteTaking/master/pictures/09_1.png) ![2](https://raw.githubusercontent.com/WingML/DataAnalysis-NoteTaking/master/pictures/09_2.png) ![3](https://raw.githubusercontent.com/WingML/DataAnalysis-NoteTaking/master/pictures/09_3.png)

___

# 用 Python 自动下载王祖贤的海报

## 流程
1. 打开网页：Requests，得到 HTML 或 JSON 数据
2. 提取数据：对于 HTML，用 Xpath 进行元素定位；对于 JSON，直接解析
3. 保存数据：Pandas

## Requests 基础用法
- GET：result = requests.get(your_url)；可用 result.text/.content 来获取 HTML 正文
- POST：result = requests.post(url, data={key: vale})

## XPath 定位：XML 的路径语言
- node：选取 node 下所有子节点
- /：从根节点选取
- //：选取所有符合的节点
- .：当前节点
- ..：父节点
- @：选择对应节点的某个属性
- |：或
- text()：当前路径下的文本内容
- 举例：xpath("./div[3]/div[@attr='my_attr']") 选择当前节点下的【第三个】div 节点下的属性名满足【attr='my_attr'】的 div 节点；xpath("...") 回到上一节点；xpath("//book/title | //book/title") 选择所有 book 元素下的 title 和 price 元素
- 在 python 中可使用解析库 lxml.etree.HTML.xpath
```python
from lxml import etree
html = etree.HTML(your_html)
result = html.xpath("//li")
```

## JSON 对象
- json.dumps()：python -> JSON
- json.loads(): JSON -> python

## 实战

### 使用 JSON 数据自动下载王祖贤的海报

```python
# coding: utf-8

import requests, json

query = "王祖贤"

def download_pics(src, p_id):
''' 下载指定图片 '''

save_dir = './' + str(p_id) + '.jpg'
try:
    pic = requests.get(src, timeout=10)
    with open(save_dir, 'wb') as fout:
        fout.write(pic.content)
except requests.exceptions.ConnectionError:
    print('无法连接到图片 %d' % p_id)

return True

### request and download all pictures with for loop
# for i in range(0, 25118, 20):
for i in range(0, 100, 20): # try a small number merely for test
# the following url is gotten from 开发者模式-Network-XHR
url = "https://www.douban.com/j/search_photo?q={0}&limit=20&start={1}".format(query, i)
html = requests.get(url).text # get json data from XHR
response = json.loads(html, encoding='utf-8') # transform json into dict

for image in response['images']:
    print("Downloading picture from {}".format(image['src']))
    download_pics(image['src'], image['id'])
```

### 使用 XPath 下载宫琦骏老爷子的电影海报封面

```python
''' To download all pages '''

### use webdriver to simulate page loading
from selenium import webdriver
import requests
from lxml import etree
import pyprind
import time
import re

def download_pics(src, p_id):
''' 下载指定图片 '''

save_dir = 'pics/'
if isinstance(p_id, str):
    # 过滤非法的文件名符号
    p_id = re.sub(r'[\\/:?|*]+', ' ', p_id)
    save_dir += p_id + '.jpg'
else:
    save_dir += str(p_id) + '.jpg'
try:
    pic = requests.get(src, timeout=10)
    with open(save_dir, 'wb') as fout:
        fout.write(pic.content)
except requests.exceptions.ConnectionError:
    print('无法连接到图片 %d' % p_id)

return True

request_url = "https://movie.douban.com/subject_search?search_text=%E5%AE%AB%E5%B4%8E%E9%AA%8F&cat=1002"
driver = webdriver.Chrome()

# 通过观察不同页码下的网页地址，大概有140个搜索结果
for i in range(0, 136, 15):
driver.get(request_url + "&start=" + str(i))
driver.implicitly_wait(10)
# 一个小小的防被封 IP 措施
time.sleep(3)

# get the source HTMl text, note that there are other ways to achieve the same goal
html = driver.execute_script("return document.documentElement.outerHTML")
html = etree.HTML(html)

src_xpath = "//div[@class='item-root']/a/img/@src"
title_xpath = "//div/div/div/a[@class='title-text']"
srcs = html.xpath(src_xpath)
titles = html.xpath(title_xpath)

# 一个用来显示进度百分数的包
pper = pyprind.ProgPercent(len(srcs))
for src, title in zip(srcs, titles):
    download_pics(src, title.text)
    pper.update()

```
